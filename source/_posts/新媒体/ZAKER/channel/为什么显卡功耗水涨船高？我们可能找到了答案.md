
---
title: '为什么显卡功耗水涨船高？我们可能找到了答案'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202201/61f12cd88e9f0965036eab2f_1024.jpg'
author: ZAKER
comments: false
date: Thu, 27 Jan 2022 05:30:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202201/61f12cd88e9f0965036eab2f_1024.jpg'
---

<div>   
<p>众所周知，如今对于整个 PC 行业来说，" 显卡 " 无疑已经成为了消费者最大的痛点。一方面是层出不穷的新品，在性能、技术上确实进步极大，老款显卡在新系统与新游戏里的性能差距被越拉越大，" 换卡 " 的需求客观存在；另一方面则是严重的缺货和猖獗的黄牛，使得真正的玩家几乎不可能用原价买到最近两三年的新卡，要么加价、要么凭运气抢购也几乎成为了显卡市场的常态。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202201/61f12cd88e9f0965036eab2f_1024.jpg" data-height="451" data-width="750" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202201/61f12cd88e9f0965036eab2f_1024.jpg" referrerpolicy="no-referrer"></div></div>不过除了以上这两点外，在最近这一两代新款显卡的设计上，所标定的功耗也已经普遍 " 水涨船高 "。举例而言，此前发布的 NVIDIA GeForce RTX3090，功耗已经比上代旗舰 TITAN RTX（当然也包括 2080Ti）高出了不少。而根据相关信息显示，前段时间刚刚亮相、至今尚未完全上市的 RTX3090Ti，标定功耗还会继续显著增加。<p></p><p>然而这还没完，近日有多方面消息显示，NVIDIA 下代旗舰显卡的算力和功耗都将有极大幅度的提升。其主频将可能达到 2500MHz，单精度算力则会从 3090Ti 的 40TFlops 暴增至 90TFlops、提升超过一倍，但同时 TDP 也会来到 550W、甚至 600W。换而言之，要想升级到下一代的旗舰显卡，1000W 的电源功率基本已经成为了底线。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202201/61f12cd88e9f0965036eab30_1024.jpg" data-height="500" data-width="750" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202201/61f12cd88e9f0965036eab30_1024.jpg" referrerpolicy="no-referrer"></div></div>回溯历史不难发现，旗舰显卡的功耗其实并不是一直在上涨。例如很典型的例子就是曾经经典的 GTX680，在性能显著提升的同时，功耗也比上一代的 GTX580 大幅下降。而纵观 GTX680 之后的多代显卡产品，从开普勒架构到伏打架构，整整四代顶级旗舰显卡的功耗其实也都一直维持在 250W 的水准，并未发生太多改变。<p></p><p>那么为什么最近这几年的顶级显卡，功耗就突然 " 膨胀 " 起来了呢？为了探究这个问题，我们三易生活统计了从 GTX680 到目前已有较为详细信息的 RTX4090 规格，并手动计算了其每 W 性能、每 MHz 性能、每核心性能等指标，也发现了一些很有趣的数据。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202201/61f12cd88e9f0965036eab31_1024.jpg" data-height="496" data-width="750" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202201/61f12cd88e9f0965036eab31_1024.jpg" referrerpolicy="no-referrer"></div></div>从这张表中可以看到，从多年前的开普勒架构到尚未发布的洛夫莱斯架构，它们的核心数量、主频和功耗都呈现出逐渐上涨的趋势。<p></p><p>在这个过程中，如果只看 " 能效比 "、也就是单纯只把算力除以功耗的话，那么除了 TITAN RTX、其他旗舰卡的能效比，客观上来说还是一直在提升的。也就是说，虽然这两三年来旗舰显卡是越来越耗电了，但它们的总体性能进步速度，还是大于功耗增加的速度。</p><p>但如果更进一步地将能效比细化到每一个核心、每 1MHz 的主频，那么情况就会完全不同了。</p><p>首先，从 GTX680 至今的旗舰显卡，每一个核心的 " 单位能效比 " 其实呈现出了一个先下降、再上升，并且最近两三代产品又开始滑坡的趋势。说的更直白点，就是如今的新架构虽然整体能效比有上升，但单核心的能效比其实是退步了的。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202201/61f12cd88e9f0965036eab32_1024.jpg" data-height="470" data-width="750" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202201/61f12cd88e9f0965036eab32_1024.jpg" referrerpolicy="no-referrer"></div></div>其次，如果将核心数量与主频的因素全部排除，更是会发现，从多年前的 GTX680 系列到尚未发布的 RTX4090，具体到每一个核心的每时钟周期性能，更是几乎完全是在 " 原地踏步 "。<p></p><p>这意味着，不同于 CPU 领域每年都要强调 IPC（每时钟周期）性能改进的做法，在 GPU 领域每一代产品的性能 " 大幅提升 "，几乎完全是靠着更多的核心数量，以及更高的运行主频来 " 堆 " 出来的。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202201/61f12cd88e9f0965036eab33_1024.jpg" data-height="422" data-width="750" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202201/61f12cd88e9f0965036eab33_1024.jpg" referrerpolicy="no-referrer"></div></div>CPU 的核心数量毕竟还是少，因此 IPC 改进就显得非常重要<p></p><p>在这种情况下，一旦制程的进步不能完全抵消更多核心、更高主频所带来的功耗增长幅度，那么最终的结果自然就是整张显卡的功耗需求 " 水涨船高 "。</p><p>至于为什么显卡里的单核心 IPC 会这么多年都 " 原地踏步 "，甚至单核心的能效比还多次出现退步，这背后的原因，就只有相关研发人员才知道了。</p><p>【本文图片来自网络】</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            