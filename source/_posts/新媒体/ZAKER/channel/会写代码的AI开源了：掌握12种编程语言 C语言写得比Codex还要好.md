
---
title: '会写代码的AI开源了：掌握12种编程语言 C语言写得比Codex还要好'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c72_1024.jpg'
author: ZAKER
comments: false
date: Wed, 09 Mar 2022 01:24:29 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c72_1024.jpg'
---

<div>   
<p>比 Codex 还会写 C 语言的 AI 代码生成模型，现在开源了！这段时间，用 AI 写代码可以说是大火，其中最著名的要属 OpenAI 的 Codex 和 DeepMind 的 AlphaCode。然而，这两个 AI 模型，全都没有开源：其中 AlphaCode 只给出了一些测试样例，而 Codex 只开放了 API。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_0" data-original="http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c72_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c72_raw.gif" data-height="334" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c72_1024.jpg" referrerpolicy="no-referrer"></div></div>△基于 Codex 的 Copilot<p></p><p>为此，来自 CMU 的几个研究人员，用 GPT-2 搞出了一个名叫<strong>PolyCoder</strong>的 AI 代码生成模型，而且还是<strong>开源的</strong>。</p><p>据研究人员表示，虽然 PolyCoder 最大只有 27 亿参数（相比 Codex 有 120 亿参数），但它用<strong>C 语言</strong>写出来的代码，比 Codex 的效果还要好。</p><p>这里面究竟有什么秘诀？</p><p>用 12 种编程语言代码集训练</p><p>首先来看训练用的<strong>数据集</strong>，这也是 PolyCoder 的最大特点之一。</p><p>此前，包括 Codex、CodeParrot 等 AI 代码生成模型，主要都是基于<strong>Python</strong>语言的代码来训练。</p><p>例如 Codex 的评估数据集之一 HumanEval，评估的也是生成 Python 代码的效果。</p><p>相比之下，<strong>PolyCoder</strong>采用了<strong>多种编程语言</strong>代码集来训练，一共有 12 种：</p><p>C、C#、C++、Go、Java、JavaScript、PHP、Python、Ruby、Rust、Scala 和 TypeScript。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c73_1024.jpg" data-height="310" data-width="660" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c73_1024.jpg" referrerpolicy="no-referrer"></div></div>其中，C 语言的代码量是最多的，达到了 221GB；而 Python 代码的数据量比 Codex 和 CodeParrot 用得都要少。<p></p><p>这里 PolyCoder 用的是 GitHub 上的公开代码，主要选取的是各种编程语言中比较受欢迎的库，每个库至少有 50 Stars。</p><p>据研究人员表示，每种编程语言库的 Stars 总数加起来不超过 25k，以避免模型生成的代码效果太过于倾斜最流行的编程语言（通常编程语言越流行，库的 Stars 就越多）。</p><p>通过提取库中的文件、经过简单处理（包括消除重复代码）后，一共筛选出大约<strong>254GB</strong>的数据用于训练。</p><p>然后是<strong>预训练</strong>的方法。</p><p>语言模型的预训练方法通常有三种。</p><p>第一种是自左向右的语言模型，根据上文预测下文，比较适用于<strong>代码生成</strong>等；第二种是掩蔽语言模型，基于上下文预测屏蔽片段，比较适合<strong>代码分类</strong>等；第三种是编解码器模型，比较适用于<strong>代码注释</strong>等任务。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c74_1024.jpg" data-height="134" data-width="660" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c74_1024.jpg" referrerpolicy="no-referrer"></div></div>这里 PolyCoder 主要采用的是第一种预训练方法。<p></p><p>相比于同样采用 GPT-2 训练的 CodeParrot 和 Codex，PolyCoder 在超参数设置上也稍微有一些差异：</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c75_1024.jpg" data-height="280" data-width="660" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c75_1024.jpg" referrerpolicy="no-referrer"></div></div>PolyCoder 一共提供了三种不同的模型，分别有 27 亿参数、4 亿参数和 1.6 亿参数，研究人员可以根据自身需求和不同的训练能力来选取合适的模型。<p></p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c76_1024.jpg" data-height="291" data-width="660" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c76_1024.jpg" referrerpolicy="no-referrer"></div></div>那么，最终训练出来的 AI 模型，代码生成效果如何？<p></p><p>C 语言写得尤其好，但 Python 不行</p><p>研究人员将 PolyCoder 与已有的 AI 代码生成模型进行了对比。</p><p>由于 AlphaCode 不好比较（接口没开放），所以研究人员主要分析了下面这些模型，包括 GPT-Neo、CodeParrot 和 Codex 等。</p><p>其中蓝色的是开源的，橙色的是没开源的：</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c77_1024.jpg" data-height="243" data-width="660" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c77_1024.jpg" referrerpolicy="no-referrer"></div></div>从参数量来看，PolyCoder 并不是最顶尖的，最大的 27 亿参数模型也只有 Codex 的四分之一不到。<p></p><p>研究人员先是用语言模型评估常用的<strong>困惑度</strong>对一系列模型进行了比较。</p><p>困惑度（Perplexity），用于衡量语言模型（LM）的好坏。困惑度越低，语言模型面对代码感到困惑的程度就越低，模型生成效果越好。</p><p>从图中来看，PolyCoder 在<strong>C 语言</strong>中意外取得了最好的效果（困惑度最低）。</p><p>用大量 C 语言训练 PolyCoder 的结果说明，即使模型整体原理不变（基于 GPT-2），单纯改变训练用的代码集，也能训练出擅长不同语言风格的 AI 代码生成模型。</p><p>可惜的是，从其他语言来看，生成的效果就完全没办法和 Codex 相比了：</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c78_1024.jpg" data-height="224" data-width="660" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c78_1024.jpg" referrerpolicy="no-referrer"></div></div>例如，在主要用于评估 Python 代码的 HumanEval 上，PolyCoder 的能力远不如 Codex 好：<p></p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c79_1024.jpg" data-height="229" data-width="660" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c79_1024.jpg" referrerpolicy="no-referrer"></div></div>据论文分析，这可能是 Python 代码数据量、模型参数量不足等原因导致的。<p></p><p>此外，作者们也提到，做出 PolyCoder 的目的主要还是为了开源一个 AI 代码生成模型，让更多人参与研究和使用。</p><p>目前代码已经开源，无论是直接拿来用，还是试着在它的基础上开发新模型都可以。</p><p>感兴趣的小伙伴可以上手一试了 ~</p><p>作者介绍</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c7a_1024.jpg" data-height="328" data-width="330" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c7a_1024.jpg" referrerpolicy="no-referrer"></div></div>一作许方正（Frank Xu），目前在 CMU 读博，研究方向是 NLP、信息抽取等，发表过多篇顶会论文，包括 ICLR、ACL 和 EMNLP 等。本硕毕业于上海交通大学，师从朱其立教授。<p></p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c7b_1024.jpg" data-height="480" data-width="316" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c7b_1024.jpg" referrerpolicy="no-referrer"></div></div>Uri Alon，在 CMU 进行博士后工作，研究方向是编程语言处理（PLP）、NLP 和深度学习。<p></p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c7c_1024.jpg" data-height="380" data-width="308" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62285c25b15ec047395c8c7c_1024.jpg" referrerpolicy="no-referrer"></div></div>Graham Neubig，CMU 助理教授，研究方向是 NLP、机器翻译和基于机器学习的自然语言理解。<p></p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c7d_1024.jpg" data-height="342" data-width="346" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62285c25b15ec047395c8c7d_1024.jpg" referrerpolicy="no-referrer"></div></div>Vincent J. Hellendoorn，CMU 计算机助理教授，主要研究方向是软件工程和机器学习，致力于利用智能方法帮助软件开发人员减少代码调试、程序优化等繁琐工作的时间。<p></p><p>不知道作者们是否已经在用这个 AI 撸代码了（手动狗头）</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            