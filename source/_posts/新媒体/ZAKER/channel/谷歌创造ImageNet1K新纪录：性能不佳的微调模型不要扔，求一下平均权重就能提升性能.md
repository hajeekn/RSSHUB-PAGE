
---
title: '谷歌创造ImageNet1K新纪录：性能不佳的微调模型不要扔，求一下平均权重就能提升性能'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdca_1024.jpg'
author: ZAKER
comments: false
date: Tue, 15 Mar 2022 23:26:01 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdca_1024.jpg'
---

<div>   
<p>如何最大限度地<strong>提升模型精度</strong>？</p><p>最近，谷歌等机构发现：</p><p>性能不好的微调模型先不要扔，求一下<strong>平均</strong><strong>权重！</strong></p><p>就能在<strong>不增加推理时间以及内存开销</strong>的情况下，提高模型的准确性和鲁棒性。</p><p>比如，研究人员就使用该方法<strong>创造了 ImageNet1K 的新纪录</strong>：90.94%。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdca_1024.jpg" data-height="346" data-width="656" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdca_1024.jpg" referrerpolicy="no-referrer"></div></div>将它扩展到多个图像分类以及自然语言处理任务中，也能提高模型的分布外性能，并改善新下游任务的零样本性能。<p></p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdcb_1024.jpg" data-height="655" data-width="500" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdcb_1024.jpg" referrerpolicy="no-referrer"></div></div>而这个方法还有一个有趣的名字，叫<strong>Module soup</strong>——<p></p><p>是不是让人一下子就让人联想到了<strong>斐波那契汤的笑话</strong><strong>？</strong>（昨天的汤 + 前天的汤 = 今天的新汤）</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdcc_1024.jpg" data-height="460" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdcc_1024.jpg" referrerpolicy="no-referrer"></div></div><b><strong>△</strong> 知乎网友 @hzwer，已授权一共三种配方</b><p></p><p>回想一下在此之前，大家是如何给模型涨点的呢？</p><p>是不是先用各种超参数训练出多个微调模型，然后再<strong>挑出验证集上表现最好的那一个留下，其余丢掉</strong><strong>？</strong></p><p>由于神经网络是非线性的，在不同的 loss basin 中可能有许多解，因此 Module soup 这一采用保留所有微调模型的权重，对其进行平均的方法就可以提高性能，还是让人有点惊讶的。</p><p>不过，最近就已有研究发现，从相同的初始化配置中中独立优化的微调模型，<strong>位于相同的误差范围内</strong>（lie in the same basin of the error landscape）。</p><p>之前也有研究证明，沿单个训练轨迹进行权重平均，可以提高随机初始化训练模型的性能。</p><p>作者正是从这些结论中受到启发。</p><p>Module soup 一共有三种 " 配方 "（实现）：统一汤（uniform soup）、贪婪汤（greedy soup）和学习汤（learned soup）。</p><p>其中<strong>greedy soup</strong>是最主要采用的实现，因为它的性能比直接均匀地平均所有权重更高。</p><p>具体来说，Greedy soup 通过<strong>顺序添加</strong>每个模型作为 " 汤 " 中的潜在成分构建而成，并且只有在保持验证集上的性能有所提高时才将相应模型保留在 " 汤 " 中。</p><p>排序按验证集精度的降序排列。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdcd_1024.jpg" data-height="217" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdcd_1024.jpg" referrerpolicy="no-referrer"></div></div><b>性能超越单个最佳微调模型</b><p></p><p>作者进行了全面的微调实验来确定 Module soup 的有效性。</p><p>首先是微调<strong>CLIP 和 ALIGN</strong>，这两个模型在图像 - 文本对上进行了对比损失预训练。</p><p>结果经过 module soup 操作后，两者在分布内和自然分布转移（distribution shifts）测试集上的表现都比最佳的单个微调模型性能更佳。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202203/6231801f8e9f092f796bfdce_1024.jpg" data-height="454" data-width="1056" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/6231801f8e9f092f796bfdce_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>△</strong> 左为 CLIP，右为 ALIGN<p></p><p>然后是在 JFT 数据集上预训练的<strong>ViT-G</strong>模型。</p><p>也就是它在 ImageNet1K 数据集实现了 90.94% 的精度，打破了此前 CoAtNet 保持的 90.88%，同时在推理阶段还减少了 25% 的 FLOPs。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdcf_1024.jpg" data-height="282" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdcf_1024.jpg" referrerpolicy="no-referrer"></div></div>在图像分类任务以外，作者在<strong>NLP</strong>领域也对 module soup 进行了验证。<p></p><p>下表是 BERT 和 T5 模型在 GLUE benchmark 的四个文本分类任务上的结果：</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdd0_1024.jpg" data-height="186" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/6231801f8e9f092f796bfdd0_1024.jpg" referrerpolicy="no-referrer"></div></div>可以发现，虽然改进<strong>不如图像分类中的效果明显</strong>，但在多数任务下，greedy soup 都可以相较最好的单个模型提高性能。<p></p><p>当然，作者也指出，module soup 在<strong>适用性等方面存在局限</strong>，比如现在测试的都是在大型异构数据集上预先训练的模型，在这些模型之外，效果并不是非常明显。</p><p>最后，知乎网友 @宫酱手艺人表示，其实这样的模型参数平均是一个经典 trick，transformer 原始论文就用了。</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres1.myzaker.com/202203/6231801f8e9f092f796bfdd1_1024.jpg" data-height="416" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/6231801f8e9f092f796bfdd1_1024.jpg" referrerpolicy="no-referrer"></div></div>你发现了吗？<p></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            