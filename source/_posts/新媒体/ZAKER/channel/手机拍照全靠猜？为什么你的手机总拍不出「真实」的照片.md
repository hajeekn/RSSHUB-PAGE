
---
title: '手机拍照全靠猜？为什么你的手机总拍不出「真实」的照片'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/6215d2128e9f097994222fce_1024.jpg'
author: ZAKER
comments: false
date: Wed, 23 Feb 2022 00:01:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/6215d2128e9f097994222fce_1024.jpg'
---

<div>   
<p>时间拉回到 2021 年 12 月，少数派团队在大理团建的那一周，几乎每天晚上都会有人跑上露台，尝试用各种各样的设备记录洱海上方那片澄澈的星空。</p><p>在这当中，几台 Google Pixel 设备的表现最为亮眼。以往我们总在感叹计算摄影的魅力，但只有用白天刷微博、刷抖音的那台手机捕捉到璀璨星河的那一刻，才真正明白算法的精妙与魅力。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres1.myzaker.com/202202/6215d2128e9f097994222fce_1024.jpg" data-height="810" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/6215d2128e9f097994222fce_1024.jpg" referrerpolicy="no-referrer"></div></div>惊叹之余，团队里一直主力使用 Google Pixel 的几位编辑也跟我聊到过一些有意思的细节：在计算摄影这件事情上，即便是一直以来被业界奉为圭臬的 Google Pixel，其实也有一些至今难以克服的短板。这当中最具代表性问题就包括偏色，这是一个从 Google Pixel 4 开始就有提及、但一直到最近的 Google Pixel 6 都没能得到有效解决的问题。<p></p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202202/6215d2128e9f097994222fcf_1024.jpg" data-height="404" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/6215d2128e9f097994222fcf_1024.jpg" referrerpolicy="no-referrer"></div></div>夜视算法加持下（右）画面色彩相比肉眼观感（左）存在明显偏色 | 图：Clyde 使用 Google Pixel 4 拍摄<p></p><p>即便是开头提到的那些星空样张也一样——对 Google 这样的计算摄影老牌劲旅来说，色彩还原也是个难以完全靠算法来解决的难题。为了让算出来的星空更加符合观感审美，在 Google 相册中的图片修饰功能中，Google 甚至还为夜视模式的照片准备了一套「天文滤镜」用于后期色彩修饰。</p><p>那色彩还原这件事为什么这么难？</p><p><strong>▍</strong><strong>问题的根源在于「猜」</strong></p><p>如果往希腊语词根的方向去理解，照片的英文是 Photograph 按字面意思可以被理解为「用光所作的画」。在摄影这件事情上，最初我们用来「作画」的载体是胶片，记录在底片上的光线信息需要经过化学冲洗才能显影；以光电二极管为核心的 CCD 与 CMOS 出现后，效率更高的光电信号转换则成为了主流。</p><p>但无论是 CCD 还是智能手机中更常见的 CMOS，受光电二极管的工作原理所限，它们都只能根据光照强度记录不同的亮度信息。换句话说，通过传感器所接收到的光电信号，仅仅只能还原出一张黑白照片。</p><p><strong>为了让传感器捕捉到有色彩的光照信息，色彩滤波阵列（CFA）出现了</strong>。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres1.myzaker.com/202202/6215d2128e9f097994222fd0_1024.jpg" data-height="702" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/6215d2128e9f097994222fd0_1024.jpg" referrerpolicy="no-referrer"></div></div>常见的色彩滤波阵列：拜尔阵列 | 图：维基百科<p></p><p>你可以将 CFA 理解为光线在到达传感器之前的一层「滤镜」，以应用最为广泛的拜尔阵列滤色镜为例，拜尔阵列以 2 个绿色像素、1 个红色像素和 1 个蓝色像素为单元，通过滤镜的光线除了能够在传感器上留下强度信息外，还能同时保留对应的色彩信息。值得注意的是，因为每个像素只过滤并记录红、绿、蓝三种颜色中的一种，这些从单个像素获取的信息其实也不能完整反映画面中不同颜色的组成方式。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202202/6215d2128e9f097994222fd1_1024.jpg" data-height="691" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/6215d2128e9f097994222fd1_1024.jpg" referrerpolicy="no-referrer"></div></div>光线通过阵列后留下不同色彩信息 | 图：维基百科<p></p><p><strong>换句话说，画面细节的丢失从我们按下手机快门键的那一刻就已经开始了</strong>。为了得到一张色彩完整的图像，通过拜尔阵列获取到的色彩信息后续还会依靠去马赛克算法进行插值、重建。如果像下图这样采用邻近像素平均值的算法，根据已采集像素对周边缺失的像素色彩信息进行推测，完成整个去马赛克过程后，重建得到的画面甚至会占据整个画面的 2/3。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202202/6215d2128e9f097994222fd2_1024.jpg" data-height="291" data-width="640" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/6215d2128e9f097994222fd2_1024.jpg" referrerpolicy="no-referrer"></div></div>将拜尔阵列得到的色彩信息还原的过程，也是细节丢失的过程 | 图：Google<p></p><p>因为「猜」的成分不可避免，最终得到的图像一般而言是无法准确还原原始拍摄场景的。即便在邻近像素平均值算法的基础之上，后续也衍生出了许多更为复杂的去马赛克算法，但「通过 CFA 获取到的色彩信息有限」这个大前提始终摆在这里。</p><p>另一方面，尽管近年很多厂商都在尝试其它的 CFA 排列方式，比如华为的 RYYB、OPPO 在 2014 年基于 QuadBayer 阵列的 RGBW，这些排列方式相比传统的拜尔阵列更多地也是在提高传感器进光量，符合前几年手机厂商在夜景拍摄这件事情上「血腥厮杀」的目标，但从实际的情况来看，纯粹靠算法猜，对解决色彩还原问题这件事并没有多大帮助。</p><p><strong>▍</strong><strong>以 Google Pixel 为鉴</strong></p><p>明确「因为照片都是猜的所以色彩还原很难」这个基本事实后，我们再回到文章开头提到的手机计算摄影巨擘 Google Pixel。如果要说 Google Pixel 诞生后这 6 年时间里，为手机行业计算摄影发展留下的、最为宝贵的经验是什么，个人认为有两点：<strong>利用好一切可用算法资源的变通性，以及垂直整合硬件的能力</strong>。</p><p>关于第一点，最好的例子是 Google Pixel 3 时引入的 Super Res Zoom（超分） —— 就是 Google 还在用单摄「吊打」其他厂商双摄长焦的那一代。</p><p>关联阅读：媲美双摄的两倍数字变焦，Google 的 Super Res Zoom 技术是这样做到的</p><p>在解决数字变焦对画面进行裁剪、重建进而造成细节丢失这个问题时，Google 相机团队首先想到的切入点也是上文提到的 CFA 滤镜。不过相比于其他同行，Google 的工程师选择了 DRIZZLE —— 一种在天文摄影学中已经流行了数十年的常用拍摄技巧。DRIZZLE 通过捕捉并合成多张拍摄角度略有变化的照片，来实现 2x 甚至 3x 的数字变焦效果，基本理念是将多张低分辨率的连拍照片直接合并对齐到更高分辨率的像素网格中。具体的合并对齐流程如下：</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202202/6215d2128e9f097994222fd3_1024.jpg" data-height="564" data-width="640" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/6215d2128e9f097994222fd3_1024.jpg" referrerpolicy="no-referrer"></div></div>通过平移的方法进行像素补全 | 图：Google<p></p><p><strong>至于垂直整合硬件的能力，大家更是有目共睹了</strong>。</p><p>从第二代 Google Pixel 开始，几乎每一代 Google Pixel 设备（a 系列以及 Pixel 5 除外）都会搭载专门用于图像算法处理的独立芯片，在 Google Pixel 2 上这枚芯片叫做 Pixel Visual Core，在 Pixel 4 上演变成为 Pixel Neural Core，最后来到今年的「大招」—— Google Tensor。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            