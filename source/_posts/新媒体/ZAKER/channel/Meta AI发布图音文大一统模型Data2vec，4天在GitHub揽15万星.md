
---
title: 'Meta AI发布图音文大一统模型Data2vec，4天在GitHub揽1.5万星'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf62_1024.jpg'
author: ZAKER
comments: false
date: Sat, 29 Jan 2022 03:14:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf62_1024.jpg'
---

<div>   
<p>Meta AI 搞了一个大一统的自监督学习模型<strong>Data2vec</strong>。</p><p>怎么个大一统法？</p><p>图像、语音、文本都可以处理，效果还都不错，在 CV 方面甚至超过了包括 MAE、MaskFeat 在内的一众模型。</p><p>这是怎么做到的？我们来看看 Data2vec 的思路和结构。</p><p>Data2vec 如何统一图音文</p><p>关于这个问题，我们可以从模型名字中看出一些端倪。</p><p>和 Word2vec 把词转化为可计算的向量类似，Data2vec 会把不同类型的数据都转化为同一种形式的数据序列。</p><p>这样就成功避开了模态不同这个问题。</p><p>然后，再用自监督学习的方法遮住这些数据的一部分，通过训练让模型把遮住的部分还原。</p><p>而它的结构也是在这个思路上设计的。</p><p>Data2vec 以 Transformer 架构为基础，设计了一个教师 - 学生网络结构：</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_0" data-original="http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf62_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf62_raw.gif" data-height="282" data-width="502" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf62_1024.jpg" referrerpolicy="no-referrer"></div></div>从上图中可以看出，无论对于任何形式的输入，都先转化为数据序列，并 mask 一部分信息（或挡住狗头，或覆盖一段语音，或遮住一个单词）。<p></p><p>然后让学生网络通过部分可见的输入去预测完整输入，再由教师网络去调整，达到一个模型处理多任务的效果。</p><p>那接下来的问题就是如何把不同类型的输入都转化为同一种形式了。</p><p>Data2vec 如何标准化输入数据</p><p>在标准化输入这一块，Data2vec 还是具体问题具体分析的。</p><p>毕竟像素、波形和文本是完全不同的形式，而 Data2vec 对不同形式的输入采用了不同的编码策略，但是目的都是一样的。</p><p>那就是将这些输入都转化为数据序列。</p><p>具体的操作方法是这样的：</p><p>任务</p><p>编码方式</p><p>掩码方式</p><p>计算机视觉</p><p>ViT 图像分块</p><p>Block-wise Masking Strategy 语音</p><p>多层一维卷积神经网络</p><p>Mask spans of latent speech representation 文本 预处理获得子词单元，然后通过嵌入向量将其嵌入分布空间</p><p>Tokens</p><p>其中 ViT 的编码策略就是把一张图分成一系列的图块，每个图块有 16x16 个像素，然后输入到一个线性变换系统中。</p><p>而语音的编码方式是用多层的一维卷积神经网络将 16kHz 的波形转换为 50Hz 的一串数据序列。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202201/61f4c2288e9f094f5931cf63_1024.jpg" data-height="582" data-width="1094" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202201/61f4c2288e9f094f5931cf63_1024.jpg" referrerpolicy="no-referrer"></div></div>再加上文本编码的嵌入向量，这样所有模态的输入都转换为了数据序列，方便后续的训练。<p></p><p>而对于掩码策略来说，不同的模态的表现形式也是不一样的。</p><p>例如图像可以遮住一块，但是语音和文本有上下文的关联，不能随便遮住一部分。</p><p>因此对不同的模态，Data2vec 也采取了相应的符合不同数据特征的掩码方式。</p><p>这样标准化之后，Data2vec 还针对不同的下游任务做了一些微调，其中语音和文本的模型已经在 GitHub 上放出，视觉模型也正在路上：</p><p>我们来看看这统一的模型性能怎么样。</p><p>性能表现</p><p>虽然 Data2vec 三手齐抓，但是性能也没落下。</p><p>在计算机视觉方面，在 IN1K 上预训练情况如下表所示：</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf65_1024.jpg" data-height="412" data-width="720" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf65_1024.jpg" referrerpolicy="no-referrer"></div></div>和一些其他模型相比，Data2vec 精度表现最好。而且 Data2vec 只训练了 800 个 epochs，而表中的 MAE，MaskFeat 训练了 1600 个 epochs。<p></p><p>看柱状图则更为明显，蓝色为 Data2vec：</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf66_1024.jpg" data-height="1080" data-width="1920" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf66_1024.jpg" referrerpolicy="no-referrer"></div></div>在语音处理方面，在 LS-960 上预训练结果如下：<p></p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202201/61f4c2288e9f094f5931cf67_1024.jpg" data-height="356" data-width="1348" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202201/61f4c2288e9f094f5931cf67_1024.jpg" referrerpolicy="no-referrer"></div></div>可以看出，Data2vec 在不同的标签数据量下单词错误率都比 wav2vec2.0 和 HuBERT 要低。<p></p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf68_1024.jpg" data-height="1080" data-width="1920" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202201/61f4c2288e9f094f5931cf68_1024.jpg" referrerpolicy="no-referrer"></div></div>而在文本处理上，Data2vec 采用了和 BERT 相同的训练设置，训练集为 Books Corpus 和英文维基百科数据。<p></p><p>在 GLUE 评估中，Data2vec 在自然语言推理（MNLI、QNLI、RTE），句子相似性（MRPC、QQP、STS-B），语法（CoLA）和情绪分析（SST）等指标中和 RoBERTa 不相上下。</p><p>其中 Baseline 这一条是 RoBERTa 在和 BERT 类似的设置中的训练结果：</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres2.myzaker.com/202201/61f4c2288e9f094f5931cf69_1024.jpg" data-height="314" data-width="1498" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202201/61f4c2288e9f094f5931cf69_1024.jpg" referrerpolicy="no-referrer"></div></div>总体评分也差不多：<p></p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres2.myzaker.com/202201/61f4c2288e9f094f5931cf6a_1024.jpg" data-height="1080" data-width="1920" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202201/61f4c2288e9f094f5931cf6a_1024.jpg" referrerpolicy="no-referrer"></div></div>这么看来，统一的模型架构真的可以有效地用于多种任务模式。<p></p><p>虽然 Data2vec 在输入数据和掩码方式上还是按照不同的方法来处理，但是它仍然是探索模型统一的尝试。</p><p>或许将来会有统一的掩码策略和不同模态数据的混合数据集，做到真正的大一统。</p><p>参考链接：</p><p> [ 1 ] https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language</p><p> [ 2 ] https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text</p><p> [ 3 ] https://github.com/pytorch/fairseq/tree/main/examples/data2vec</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            