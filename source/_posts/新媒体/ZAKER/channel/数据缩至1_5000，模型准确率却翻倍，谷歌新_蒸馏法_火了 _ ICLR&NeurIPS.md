
---
title: '数据缩至1_5000，模型准确率却翻倍，谷歌新_蒸馏法_火了 _ ICLR&NeurIPS'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420d9_1024.jpg'
author: ZAKER
comments: false
date: Thu, 16 Dec 2021 23:43:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420d9_1024.jpg'
---

<div>   
<p>在炼丹过程中，为了减少训练所需资源，MLer 有时会将大型复杂的大模型 " 蒸馏 " 为较小的模型，同时还要保证与压缩前相当的结果。</p><p>这就是知识蒸馏，一种模型压缩 / 训练方法。</p><p>不过随着技术发展，大家也逐渐将蒸馏的对象扩展到了数据集上。</p><p>这不，谷歌最近就提出了两种新的数据集蒸馏方法，在推特上引起了不小反响，热度超过 600：</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420d9_1024.jpg" data-height="846" data-width="1040" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420d9_1024.jpg" referrerpolicy="no-referrer"></div></div>像这样， 将 50000 张标注图像的 CIFAR-10 数据集 " 蒸馏 " 缩小至 1/5000 大小，只基于<strong>10 张</strong>合成数据点进行训练，模型的准确率仍可近似<strong>51%</strong>：<p></p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420da_1024.jpg" data-height="254" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420da_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>△</strong>上：原始数据集 下：蒸馏后<p></p><p>而如果 " 蒸馏数据集 " 由 500 张图像组成（占原数据集 1% 大小），其准确率可以达到 80%。</p><p>两种数据集蒸馏方法分别来自于 ICLR 2021 和 NeurIPS 2021 上的两篇论文。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420db_1024.jpg" data-height="198" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420db_1024.jpg" referrerpolicy="no-referrer"></div></div><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420dc_1024.jpg" data-height="160" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420dc_1024.jpg" referrerpolicy="no-referrer"></div></div><b>通过两阶段循环进行优化</b><p></p><p>那么要如何才能 " 蒸馏 " 一个数据集呢？</p><p>其实，这相当于一个两阶段的优化过程：</p><p>" 内部循环 "，用于在学习数据上训练模型</p><p>" 外部循环 "，用于优化学习数据在自然数据上的性能</p><p>通过内部循环可以得到一个核岭回归（KRR）函数，然后再外部循环中计算原始图像标注与核岭回归函数预测标注之间的均方误差（MSE）。</p><p>这时，谷歌提出的两种方法就分别有了不同的处理路线：</p><p>一、<strong>标注解释</strong>（LS）</p><p>这种方法直接解释最小化 KRR 损失函数的支持标注集（support labels），并为每个支持图像生成一个独特的密集标注向量。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420dd_1024.jpg" data-height="1220" data-width="848" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420dd_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>△</strong>蓝：原始独热标注 橙：LS 生成的密集标注<p></p><p>二、<strong>核归纳点</strong>（KIP）</p><p>这种方法通过基于梯度的方法将 KRR 损失函数最小化，以此来优化图像和可能生成的数据。</p><p>以 MNIST 为例，下图中的上、中、下三张图分别为原始的 MNIST 数据集、固定标注的 KIP 蒸馏图像、优化标注的 KIP 蒸馏图像。</p><p>可以看出，在于对数据集进行蒸馏时，优化标注的效果最好：</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202112/61bc3ef38e9f097c696420de_1024.jpg" data-height="659" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202112/61bc3ef38e9f097c696420de_1024.jpg" referrerpolicy="no-referrer"></div></div>对比已有的 DC（Dataset Condensation）方法和 DSP（Dataset Condensation with Differentiable Siamese Augmentation）方法可以看到：<p></p><p>如果使用每类别只有一张图像，也就是最后只有 10 张图像的蒸馏数据集，KIP 方法的测试集准确率整体高于 DC 和 DSP 方法。</p><p>在 CIFAR-10 分类任务中，LS 也优于先前的方法，KIP 甚至可以达到翻倍的效果。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420df_1024.jpg" data-height="602" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202112/61bc3ef38e9f097c696420df_1024.jpg" referrerpolicy="no-referrer"></div></div>对此，谷歌表示：<p></p><p>这证明了在某些情况下，我们的缩小 100 倍的 " 蒸馏数据集 " 要比原始数据集更好。</p><p><b></b></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            